{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introbert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPoKjb1hpjuJjSwfFGYD32C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dar-tau/nlp-experiments/blob/master/introbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9lKLc_FDgfW",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqceyW70h1eK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "outputId": "eec76429-82d9-4fc9-d300-7dee81f4f619"
      },
      "source": [
        "!pip install transformers datasets\n",
        "# !pip install simpletransformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 5.1MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/f2/d213673d76ee56d907e462e6c144f1418368d35e6a9221799403116516de/datasets-1.0.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 23.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.0.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.2)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/99/0a605f016121ca314d1469dc9069e4978395bc46fda40f73099d90ad3ba4/pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 200kB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 56.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=974c1d006d217023c105c65c9b4b6029becae8d05cd4da1361f4ee0b913e2d84\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.0.1 pyarrow-1.0.1 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_82TRLIFBe6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "ed2a38ef-82d8-49c7-b6ab-7eaabc8b54f3"
      },
      "source": [
        "%cd /content\n",
        "!mkdir data\n",
        "%cd /content/data\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/data\n",
            "--2020-09-20 11:14:56--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M  96.8MB/s    in 0.4s    \n",
            "\n",
            "2020-09-20 11:14:58 (96.8 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVNwlAAYhj_5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "09f9cc76-1db5-4c3d-fdca-9ae58b3cf6c4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import json\n",
        "import os\n",
        "\n",
        "import re\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import datasets\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from transformers.data.processors.squad import SquadV2Processor, squad_convert_examples_to_features"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version 1.6.0+cu101 available.\n",
            "TensorFlow version 2.3.0 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE2GOSq--Ml1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda'\n",
        "\n",
        "def torchTokenize(*args):\n",
        "  return tokenizer(*args, truncation = True,\n",
        "                       padding = True, return_tensors = 'pt')\n",
        "\n",
        "def squad_to_introbert(squad_zipped_example_and_features):\n",
        "  squad_example, squad_features = squad_zipped_example_and_features\n",
        "  res = {'start_position': squad_features.start_position, \n",
        "         'end_position': squad_features.end_position}\n",
        "  res.update({'context': squad_example.context_text,\n",
        "              'question': squad_example.question_text})\n",
        "  # res.update({k: torch.Tensor(as_dict[k], device = device) for k in ['input_ids', 'attention_mask','token_type_ids'] })\n",
        "  return res\n",
        "\n",
        "def dictToDevice(d, device):\n",
        "  d_ = {}\n",
        "  for k, v in d.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "      d_[k] = v.to(device)\n",
        "    else:\n",
        "      d_[k] = v\n",
        "  return d_"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl0yRM_pG1r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IntrobertDataset(Dataset):\n",
        "  def __init__(self, srcDataset, func, device = device):\n",
        "    self.ds = srcDataset\n",
        "    self.func = func\n",
        "    self.device = device\n",
        "    self.isModelSet = False\n",
        "\n",
        "  def setModel(self, model, nLayers, nHeads):\n",
        "    self.model = model\n",
        "    self.nLayers = nLayers\n",
        "    self.nHeads = nHeads\n",
        "    self.isModelSet = True \n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    assert(self.isModelSet)\n",
        "    data = self.func(self.ds[i])\n",
        "    context = data['context']\n",
        "    start_position = None\n",
        "    end_position = None\n",
        "    inputs = None\n",
        "    introspection = None\n",
        "    use_original = self.choose_use_original()\n",
        "    if use_original: \n",
        "      start_position = data['start_position']\n",
        "      end_position = data['end_position']\n",
        "      question = data['question']\n",
        "    else:\n",
        "        chosenLayer = np.random.choice(self.nLayers)\n",
        "        chosenHead = np.random.choice(self.nHeads)\n",
        "        question = \"what is the most attended word in layer {} head {}?\".format(chosenLayer, chosenHead)\n",
        "\n",
        "        def introspection(model, attentions):\n",
        "          res = attentions[chosenLayer][:,chosenHead].sum(dim = -2)[:, 1:].argmax()\n",
        "          res += 1\n",
        "          return (res, res)\n",
        "        \n",
        "    inputs = torchTokenize(context,question)\n",
        "    inputs = dictToDevice(inputs, self.device)\n",
        "\n",
        "    return {'context': context, 'inputs': inputs, 'start_position' : start_position, 'end_position': end_position,\n",
        "            'question': question, 'introspection': introspection, 'use_original': use_original}\n",
        "\n",
        "  def choose_use_original(self):\n",
        "    return np.random.choice(2) == 0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ds)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlQ_81GJfR1A",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0okVrbpMfRiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\", output_attentions = True,  \n",
        "                                                      return_dict = True)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr = 3e-6)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NceD8emvJi9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'dataset' not in globals():\n",
        "  max_seq_length = 384\n",
        "  doc_stride = 128\n",
        "  max_query_length = 64\n",
        "  total = 10000\n",
        "  squad_examples = SquadV2Processor().get_train_examples(\"/content/data\")[:total]\n",
        "  squad_features = squad_convert_examples_to_features(squad_examples, tokenizer = tokenizer, \n",
        "                                    max_seq_length = max_seq_length,\n",
        "                                    max_query_length = max_query_length,\n",
        "                                    doc_stride = doc_stride, is_training = True, return_dataset = None)\n",
        "\n",
        "dataset = IntrobertDataset(list(zip(squad_examples, squad_features)), squad_to_introbert)\n",
        "dataset.setModel(\"distilbert-base-cased-distilled-squad\", 6, 12)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ogcu9qi8FWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "dbabcf5a-be4f-4073-9be1-2354163621b6"
      },
      "source": [
        "from collections import defaultdict\n",
        "start_positions = defaultdict(int)\n",
        "was_original = []\n",
        "\n",
        "n_epochs = 10\n",
        "num_training_steps = total * n_epochs \n",
        "num_warmup_steps = total \n",
        "losses = []\n",
        " \n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: x/num_training_steps)\n",
        "# get_linear_schedule_with_warmup(optimizer,num_warmup_steps, num_training_steps)\n",
        "# torch.optim.lr_scheduler.OneCycleLR(optimizer, 5e-5, total_steps = n_epochs * total,\n",
        "#                                                 epochs = n_epochs)\n",
        "\n",
        "\n",
        "from tqdm import tqdm as simple_tqdm\n",
        "model.train()\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  losses.append([])\n",
        "  t = simple_tqdm(dataset, total = total, leave = True, position = 0)\n",
        "  acc_sum1 = 0\n",
        "  acc_sum2 = 0\n",
        "  i_ = 0\n",
        "  for i, data in enumerate(t):\n",
        "    if i >= total:\n",
        "      break    \n",
        "\n",
        "    model.eval()\n",
        "    inputs = data['inputs']\n",
        "    introspection = data['introspection']\n",
        "    use_original = data['use_original']\n",
        "    was_original.append(int(use_original))\n",
        "    if not use_original:\n",
        "      outputs = model(**inputs)\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    if use_original:\n",
        "      start_position = data['start_position']\n",
        "      end_position = data['end_position']\n",
        "    else:\n",
        "      start_position, end_position = introspection(model, outputs.attentions)\n",
        "      start_positions[start_position.item()] += 1          \n",
        "    start_position = torch.Tensor([start_position]).to(device).detach().long()\n",
        "    end_position = torch.Tensor([end_position]).to(device).detach().long()\n",
        "\n",
        "    outputs = model(**inputs, start_positions = start_position,\n",
        "                    end_positions = end_position)\n",
        "    \n",
        "    loss = outputs.loss\n",
        "\n",
        "    losses[e].append(loss.item())\n",
        "\n",
        "    if not use_original:\n",
        "      acc_sum1 += int((start_position.item() == outputs.start_logits.argmax().item())) \n",
        "      acc_sum2 += int((end_position.item() == outputs.end_logits.argmax().item()))\n",
        "      i_ += 1\n",
        "      acc1 = acc_sum1/i_\n",
        "      acc2 = acc_sum2/i_\n",
        "\n",
        "    t.set_postfix_str(\"Loss: {:.2f}, Acc1: {:.2f}, Acc2: {:.2f}\".format(loss.item(), acc1, acc2))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [08:00<00:00, 20.81it/s, Loss: 0.78, Acc1: 0.23, Acc2: 0.18]\n",
            "100%|██████████| 10000/10000 [08:00<00:00, 20.79it/s, Loss: 1.68, Acc1: 0.37, Acc2: 0.37]\n",
            "100%|██████████| 10000/10000 [08:03<00:00, 20.70it/s, Loss: 0.30, Acc1: 0.39, Acc2: 0.39]\n",
            "100%|██████████| 10000/10000 [08:02<00:00, 20.74it/s, Loss: 0.76, Acc1: 0.39, Acc2: 0.39]\n",
            "100%|██████████| 10000/10000 [08:07<00:00, 20.51it/s, Loss: 0.27, Acc1: 0.39, Acc2: 0.38]\n",
            " 17%|█▋        | 1742/10000 [01:30<07:32, 18.25it/s, Loss: 0.96, Acc1: 0.38, Acc2: 0.38]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz7VV5IAgs6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydItyMS4FY5h",
        "colab_type": "text"
      },
      "source": [
        "## Old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI0OyxO7FYnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setModelHooks(model):\n",
        "  attentionLayerRegex = r'^(.+\\.)*layer\\.(\\d+)\\.attention$'\n",
        "  def _guyAttentionHook(name):\n",
        "    layerNum = int(re.match(attentionLayerRegex, name).group(2))\n",
        "    # Assumes there's only one attention per number\n",
        "    def _myHook(m, inp, outp):\n",
        "      assert((type(outp) == tuple) and (len(outp) == 1) )\n",
        "\n",
        "      model.guyData[layerNum] = F.softmax(outp[0], dim = -1)\n",
        "\n",
        "    return _myHook\n",
        "\n",
        "\n",
        "  if hasattr(model, 'guyHooks'):\n",
        "    print(\"Removing existing hooks!\")\n",
        "    [hook.remove() for hook in model.guyHooks]\n",
        "  \n",
        "  model.guyData = {}\n",
        "  model.guyHooks = [module.register_forward_hook(_guyAttentionHook(name)) for name, module in model.named_modules()\n",
        "                                                                          if re.match(attentionLayerRegex, name) is not None]\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}