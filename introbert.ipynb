{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introbert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO6iwSqRkxVZko0B43x3d4j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dar-tau/nlp-experiments/blob/master/introbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9lKLc_FDgfW",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqceyW70h1eK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers datasets\n",
        "# !pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_82TRLIFBe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content\n",
        "!mkdir data\n",
        "%cd /content/data\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVNwlAAYhj_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import json\n",
        "import os\n",
        "\n",
        "import re\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import datasets\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from transformers.data.processors.squad import SquadV2Processor, squad_convert_examples_to_features"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE2GOSq--Ml1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda'\n",
        "\n",
        "def torchTokenize(*args):\n",
        "  return tokenizer(*args, truncation = True,\n",
        "                       padding = True, return_tensors = 'pt')\n",
        "\n",
        "def squad_to_introbert(squad_zipped_example_and_features):\n",
        "  squad_example, squad_features = squad_zipped_example_and_features\n",
        "  res = {'start_position': squad_features.start_position, \n",
        "         'end_position': squad_features.end_position}\n",
        "  res.update({'context': squad_example.context_text,\n",
        "              'question': squad_example.question_text})\n",
        "  # res.update({k: torch.Tensor(as_dict[k], device = device) for k in ['input_ids', 'attention_mask','token_type_ids'] })\n",
        "  return res\n",
        "\n",
        "def dictToDevice(d, device):\n",
        "  d_ = {}\n",
        "  for k, v in d.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "      d_[k] = v.to(device)\n",
        "    else:\n",
        "      d_[k] = v\n",
        "  return d_"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl0yRM_pG1r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IntrobertDataset(Dataset):\n",
        "  def __init__(self, srcDataset, func, device = device):\n",
        "    self.ds = srcDataset\n",
        "    self.func = func\n",
        "    self.device = device\n",
        "    self.isModelSet = False\n",
        "\n",
        "  def setModel(self, model, nLayers, nHeads):\n",
        "    self.model = model\n",
        "    self.nLayers = nLayers\n",
        "    self.nHeads = nHeads\n",
        "    self.isModelSet = True \n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    assert(self.isModelSet)\n",
        "    data = self.func(self.ds[i])\n",
        "    context = data['context']\n",
        "    start_position = None\n",
        "    end_position = None\n",
        "    inputs = None\n",
        "    introspection = None\n",
        "    use_original = self.choose_use_original()\n",
        "    if use_original: \n",
        "      start_position = data['start_position']\n",
        "      end_position = data['end_position']\n",
        "      question = data['question']\n",
        "    else:\n",
        "        inputs = torchTokenize(context)\n",
        "        inputs = dictToDevice(inputs, self.device)\n",
        "\n",
        "        chosenLayer = np.random.choice(self.nLayers)\n",
        "        chosenHead = np.random.choice(self.nHeads)\n",
        "        question = \"what is the most attended word in layer {} head {}?\".format(chosenLayer, chosenHead)\n",
        "\n",
        "        def introspection(model, attentions):\n",
        "          res = attentions[chosenLayer][:,chosenHead].sum(dim = -2)[:, 1:].argmax()\n",
        "          res += 1\n",
        "          return (res, res)\n",
        "        \n",
        "    return {'context': context, 'inputs': inputs, 'start_position' : start_position, 'end_position': end_position,\n",
        "            'question': question, 'introspection': introspection, 'use_original': use_original}\n",
        "\n",
        "  def choose_use_original(self):\n",
        "    return np.random.choice(2) == 0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ds)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlQ_81GJfR1A",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0okVrbpMfRiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\", output_attentions = True,  \n",
        "                                                      return_dict = True)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr = 5e-5)\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NceD8emvJi9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'dataset' not in globals():\n",
        "  max_seq_length = 384\n",
        "  doc_stride = 128\n",
        "  max_query_length = 64\n",
        "  total = 10000\n",
        "  squad_examples = SquadV2Processor().get_train_examples(\"/content/data\")[:total]\n",
        "  squad_features = squad_convert_examples_to_features(squad_examples, tokenizer = tokenizer, \n",
        "                                    max_seq_length = max_seq_length,\n",
        "                                    max_query_length = max_query_length,\n",
        "                                    doc_stride = doc_stride, is_training = True, return_dataset = None)\n",
        "\n",
        "dataset = IntrobertDataset(list(zip(squad_examples, squad_features)), squad_to_introbert)\n",
        "dataset.setModel(model, 6, 12)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ogcu9qi8FWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "start_positions = defaultdict(int)\n",
        "was_original = []\n",
        "\n",
        "n_epochs = 10\n",
        "num_training_steps = total * n_epochs \n",
        "num_warmup_steps = total \n",
        "losses = []\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps, num_training_steps)\n",
        "\n",
        "\n",
        "from tqdm import tqdm as simple_tqdm\n",
        "model.train()\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  losses.append([])\n",
        "  t = simple_tqdm(dataset, total = total, leave = True, position = 0)\n",
        "  acc_sum1 = 0\n",
        "  acc_sum2 = 0\n",
        "  for i, data in enumerate(t):\n",
        "    if i >= total:\n",
        "      break    \n",
        "\n",
        "    model.eval()\n",
        "    context = data['context']\n",
        "    inputs = data['inputs']\n",
        "    question = data['question']\n",
        "    introspection = data['introspection']\n",
        "    use_original = data['use_original']\n",
        "    was_original.append(int(use_original))\n",
        "    if not use_original:\n",
        "      outputs = model(**inputs)\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    if use_original:\n",
        "      start_position = data['start_position']\n",
        "      end_position = data['end_position']\n",
        "    else:\n",
        "      start_position, end_position = introspection(model, outputs.attentions)\n",
        "      start_positions[start_position.item()] += 1          \n",
        "    start_position = torch.Tensor([start_position]).to(device).detach().long()\n",
        "    end_position = torch.Tensor([end_position]).to(device).detach().long()\n",
        "\n",
        "    inputs = torchTokenize(context, question)\n",
        "    inputs = dictToDevice(inputs, device)\n",
        "    outputs = model(**inputs, start_positions = start_position,\n",
        "                    end_positions = end_position)\n",
        "    \n",
        "    loss = outputs.loss\n",
        "\n",
        "    losses[e].append(loss.item())\n",
        "  \n",
        "    acc_sum1 += int((start_position.item() == outputs.start_logits.argmax().item())) \n",
        "    acc_sum2 += int((end_position.item() == outputs.end_logits.argmax().item()))\n",
        "    acc1 = acc_sum1/i if i!=0 else 0.0\n",
        "    acc2 = acc_sum2/i if i!=0 else 0.0\n",
        "\n",
        "    t.set_postfix_str(\"Loss: {:.2f}, Acc1: {:.2f}, Acc2: {:.2f}\".format(loss.item(), acc1, acc2))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydItyMS4FY5h",
        "colab_type": "text"
      },
      "source": [
        "## Old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI0OyxO7FYnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setModelHooks(model):\n",
        "  attentionLayerRegex = r'^(.+\\.)*layer\\.(\\d+)\\.attention$'\n",
        "  def _guyAttentionHook(name):\n",
        "    layerNum = int(re.match(attentionLayerRegex, name).group(2))\n",
        "    # Assumes there's only one attention per number\n",
        "    def _myHook(m, inp, outp):\n",
        "      assert((type(outp) == tuple) and (len(outp) == 1) )\n",
        "\n",
        "      model.guyData[layerNum] = F.softmax(outp[0], dim = -1)\n",
        "\n",
        "    return _myHook\n",
        "\n",
        "\n",
        "  if hasattr(model, 'guyHooks'):\n",
        "    print(\"Removing existing hooks!\")\n",
        "    [hook.remove() for hook in model.guyHooks]\n",
        "  \n",
        "  model.guyData = {}\n",
        "  model.guyHooks = [module.register_forward_hook(_guyAttentionHook(name)) for name, module in model.named_modules()\n",
        "                                                                          if re.match(attentionLayerRegex, name) is not None]\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}