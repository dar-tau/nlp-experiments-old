{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfj6qUCzeQfA"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "id": "TMUnskJ9eQfB"
   },
   "outputs": [],
   "source": [
    "storageName = 'university'\n",
    "\n",
    "if storageName == 'paperspace': \n",
    "    guy_folder = \"/notebooks/\"\n",
    "elif storageName == 'colab':\n",
    "    guy_folder = \"/content/\"\n",
    "elif storageName == 'university':\n",
    "    guy_folder = '/vol/scratch/guy/'\n",
    "    \n",
    "    \n",
    "cache_dir = guy_folder+\"/cache/transformer_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling enum34-1.1.6:\n",
      "  Successfully uninstalled enum34-1.1.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y enum34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./miniconda/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: seaborn in ./miniconda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: pandas in ./miniconda/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already satisfied: tqdm in ./miniconda/lib/python3.8/site-packages (4.46.0)\n",
      "Requirement already satisfied: tensorboard in ./miniconda/lib/python3.8/site-packages (2.3.0)\n",
      "Collecting torch\n",
      "  Using cached torch-1.6.0-cp38-cp38-manylinux1_x86_64.whl (748.8 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.7.0-cp38-cp38-manylinux1_x86_64.whl (5.9 MB)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in ./miniconda/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./miniconda/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in ./miniconda/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in ./miniconda/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./miniconda/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./miniconda/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./miniconda/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: scipy>=1.0 in ./miniconda/lib/python3.8/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./miniconda/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (1.32.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (46.4.0.post20200518)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (1.21.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (3.13.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./miniconda/lib/python3.8/site-packages (from tensorboard) (0.10.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in ./miniconda/lib/python3.8/site-packages (from tensorboard) (0.34.2)\n",
      "Processing /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4/future-0.18.2-py3-none-any.whl\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./miniconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./miniconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./miniconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in ./miniconda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./miniconda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./miniconda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./miniconda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./miniconda/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./miniconda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Installing collected packages: future, torch, torchvision\n",
      "Successfully installed future-0.18.2 torch-1.6.0 torchvision-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib seaborn pandas tqdm tensorboard torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "collapsed": false,
    "id": "NqAi8nT2eQfG",
    "outputId": "20ff04dc-620b-40f5-9427-af3192a9a444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/specific/scratches/scratch/guy\n",
      "mkdir: cannot create directory 'cache': File exists\n",
      "mkdir: cannot create directory 'cache/transformer_cache': File exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%cd {guy_folder}\n",
    "!mkdir cache\n",
    "!mkdir cache/transformer_cache\n",
    "# %pip install --no-cache-dir --upgrade torch torchvision\n",
    "#==1.4.0+cu100 torchvision== -f https://download.pytorch.org/whl/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "htC1bYBQfkz6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.10.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |################################| 1.6 MB 461 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from wandb) (1.14.0)\n",
      "Collecting Click>=7.0\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.8-py3-none-any.whl (159 kB)\n",
      "\u001b[K     |################################| 159 kB 66.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting subprocess32>=3.5.3\n",
      "  Using cached subprocess32-3.5.4.tar.gz (97 kB)\n",
      "Collecting watchdog>=0.8.3\n",
      "  Using cached watchdog-0.10.3.tar.gz (94 kB)\n",
      "Collecting configparser>=3.8.1\n",
      "  Using cached configparser-5.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Using cached shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "  Downloading sentry_sdk-0.17.8-py2.py3-none-any.whl (120 kB)\n",
      "\u001b[K     |################################| 120 kB 73.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from wandb) (3.13.0)\n",
      "Collecting PyYAML\n",
      "  Using cached PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from wandb) (2.8.1)\n",
      "Collecting psutil>=5.0.0\n",
      "  Using cached psutil-5.7.2.tar.gz (460 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "Collecting pathtools>=0.1.1\n",
      "  Using cached pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: certifi in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from sentry-sdk>=0.4.0->wandb) (2020.6.20)\n",
      "Requirement already satisfied: urllib3>=1.10.0 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from sentry-sdk>=0.4.0->wandb) (1.25.8)\n",
      "Requirement already satisfied: setuptools in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from protobuf>=3.12.0->wandb) (46.4.0.post20200518)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Using cached smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: promise, subprocess32, watchdog, PyYAML, psutil, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=819290c8635121bd8a4c487dd0a413940f798f5eaf57c65814f0e8c56d103f03\n",
      "  Stored in directory: /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6489 sha256=382216b2c3b49a0a5996cdf1333e3e9bec1ef19bfa3859472064cbb979a0ff02\n",
      "  Stored in directory: /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/9f/69/d1/50b39b308a87998eaf5c1d9095e5a5bd2ad98501e2b7936d36\n",
      "  Building wheel for watchdog (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Created wheel for watchdog: filename=watchdog-0.10.3-py3-none-any.whl size=73871 sha256=6ff5db392c434c47062913a0f1fa2fb68cc21bda31018129242e029004e7f6d9\n",
      "  Stored in directory: /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/f8/39/45/b80612a24e42d9de0bd5eaf69eaf953edc25e1a9809d3d989f\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp38-cp38-linux_x86_64.whl size=472383 sha256=7f808df76ea1eaba82ffc39486566fbeec839b641a3f063daf469a4abdde49c1\n",
      "  Stored in directory: /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/13/90/db/290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.2-cp38-cp38-linux_x86_64.whl size=268326 sha256=9ac5559f9cd49fa522a4e5d0040697415d1851c45f3a70633b76acc0a8c4d626\n",
      "  Stored in directory: /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/91/cf/b0/0c9998060b55ca80ea7a50a8639c3bdc6ba886eeff014bc9ac\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=7ac19cec92f54c6541c7cc2b870715902f60a2382d743e1f17558ace32709a17\n",
      "  Stored in directory: /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built promise subprocess32 watchdog PyYAML psutil pathtools\n",
      "Installing collected packages: Click, promise, smmap, gitdb, GitPython, docker-pycreds, subprocess32, pathtools, watchdog, configparser, shortuuid, sentry-sdk, PyYAML, psutil, wandb\n",
      "Successfully installed Click-7.1.2 GitPython-3.1.8 PyYAML-5.3.1 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 promise-2.3 psutil-5.7.2 sentry-sdk-0.17.8 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.10.2 watchdog-0.10.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/specific/scratches/scratch/guy/cache\n",
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
      "Processing ./transformers\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25hRequirement already satisfied: requests in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from transformers==3.2.0) (2.23.0)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Using cached sentencepiece-0.1.91-cp38-cp38-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Using cached tokenizers-0.8.1rc2-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "Requirement already satisfied: packaging in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from transformers==3.2.0) (20.4)\n",
      "Processing /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677/sacremoses-0.0.43-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.27 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from transformers==3.2.0) (4.46.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: numpy in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from transformers==3.2.0) (1.19.2)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.9.27-cp38-cp38-manylinux2010_x86_64.whl (675 kB)\n",
      "\u001b[K     |################################| 675 kB 431 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests->transformers==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests->transformers==3.2.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests->transformers==3.2.0) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests->transformers==3.2.0) (2.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from packaging->transformers==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: six in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from packaging->transformers==3.2.0) (1.14.0)\n",
      "Requirement already satisfied: click in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from sacremoses->transformers==3.2.0) (7.1.2)\n",
      "Collecting joblib\n",
      "  Using cached joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-3.2.0-py3-none-any.whl size=1058986 sha256=5952a311496c65f1976f9c84820f188a92a427a5d1e91eb11e6619f4383b9b09\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jmbovbqd/wheels/62/72/05/d86ccfc8a8e2bef63ff35c03f9c6ad973706febb4488bdd2f2\n",
      "Successfully built transformers\n",
      "Installing collected packages: sentencepiece, tokenizers, joblib, regex, sacremoses, filelock, transformers\n",
      "Successfully installed filelock-3.0.12 joblib-0.16.0 regex-2020.9.27 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nlp\n",
      "  Using cached nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "Processing /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/93/7f/7d/78ec535a4340ef2696aad8b17fe8bb063d56301bd62881b069/dill-0.3.2-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (4.46.0)\n",
      "Collecting pyarrow>=0.16.0\n",
      "  Using cached pyarrow-1.0.1-cp38-cp38-manylinux2014_x86_64.whl (17.3 MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (2.23.0)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-2.0.0-cp38-cp38-manylinux2010_x86_64.whl (243 kB)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from pandas->nlp) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from pandas->nlp) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.14.0)\n",
      "Installing collected packages: dill, pyarrow, xxhash, nlp\n",
      "Successfully installed dill-0.3.2 nlp-0.4.0 pyarrow-1.0.1 xxhash-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/specific/scratches/scratch/guy\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb\n",
    "%cd {guy_folder}/cache/\n",
    "!git clone https://github.com/huggingface/transformers.git\n",
    "%pip install ./transformers\n",
    "%pip install -U nlp\n",
    "\n",
    "%cd {guy_folder}\n",
    "\n",
    "## Not working!!!\n",
    "!setenv TRANSFORMERS_CACHE /vol/scratch/guy/cache/transformer_cache\n",
    "!setenv CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KPmqdmNj6dN"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "XhRzzS1eeQfZ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5513c90da879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageNet, ImageFolder, CIFAR10, CIFAR100\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet101\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "id": "sS-kATGqeQfc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DummyLayer(nn.Module):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__()\n",
    "    pass\n",
    "  def forward(self, x, *args, **kwargs):\n",
    "    return x\n",
    "\n",
    "class PlainBERT(nn.Module):\n",
    "    def __init__(self, n_tokens, min_layer = None):\n",
    "        super().__init__()\n",
    "        self.nLayers = 6\n",
    "        self.nHeads = 12\n",
    "        self.seqLen = 512\n",
    "\n",
    "\n",
    "        bert = AutoModel.from_pretrained('distilbert-base-uncased', cache_dir = cache_dir)\n",
    "        self.position_embeddings = nn.Parameter(torch.Tensor(bert.embeddings.position_embeddings(torch.arange(self.seqLen)).detach().numpy()))\n",
    "        if min_layer is None:\n",
    "          self.bert = bert.transformer\n",
    "        else:\n",
    "          raise NotImplementedError\n",
    "          bert_ = bert.transformer\n",
    "          for n, m in bert_.layer.named_children():\n",
    "            if int(n) < min_layer:\n",
    "              setattr(bert_.layer, n, DummyLayer())\n",
    "        \n",
    "          self.bert = bert_\n",
    "\n",
    "        self.bert.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bert.forward(x + self.position_embeddings, attn_mask = torch.ones(x.size(0), 512).to(x.device),\n",
    "                                head_mask = torch.ones(self.nLayers, x.size(0), self.nHeads, self.seqLen, self.seqLen).to(x.device))\n",
    "\n",
    "class BertVision(nn.Module):\n",
    "    def __init__(self,  n_classes, img_dim):\n",
    "        super().__init__()\n",
    "        self.with_classifier = True\n",
    "        self.n_tokens = np.prod(img_dim)\n",
    "        self.top = nn.Sequential(\n",
    "                                 nn.Conv2d(3, 32, 3, padding = 1 ),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(32, 100, 3, padding = 1),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(100, 200, 3, padding = 1),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(200, 768, 3, stride = (1, 2), padding = 1),\n",
    "                                 nn.LeakyReLU(0.2)\n",
    "                                )\n",
    "        \n",
    "        self.top.apply(self._init_top)\n",
    "        self.bert = PlainBERT(n_tokens = self.n_tokens)\n",
    "        self.fc = nn.Linear(768 * self.n_tokens//2, n_classes)\n",
    "        self.layer_norm = nn.LayerNorm((512,))\n",
    "\n",
    "    def toggleIntermediate(self):\n",
    "        self.with_classifier = !self.with_classifier\n",
    "    \n",
    "    \n",
    "    def _init_top(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        x = self.top(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.bert(x)\n",
    "        x = torch.stack(x).squeeze(0)\n",
    "        x = x.transpose(1,2).contiguous()\n",
    "#         x = self.layer_norm(x)\n",
    "#         x = torch.mean(x, dim = (-2,))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.with_classifier:\n",
    "            x = self.fc(x)\n",
    "        return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "-Udifw4deQfn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /notebooks//data/cifar100/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169009152it [00:09, 18300545.88it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /notebooks//data/cifar100/cifar-100-python.tar.gz to /notebooks//data/cifar100\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "train_ds = CIFAR100(\"{}/data/cifar100\".format(guy_folder), download = True, transform=transforms.ToTensor())\n",
    "test_ds = CIFAR100(\"{}/data/cifar100\".format(guy_folder), download = True, transform=transforms.ToTensor(), train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QajOFf1ic7-"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "6ixPIuePqq86"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "# After login in through firefox to ImageNet\n",
    "%cd {guy_folder}/data\n",
    "!wget http://www.image-net.org/archive/stanford/fall11_whole.tar\n",
    "# !tar -xvf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xnJGD7m7qfSw"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "train_folder = \"{}/data/imagenet12/train\".format(guy_folder)\n",
    "\n",
    "train_ds = ImageFolder(train_folder,\n",
    "            transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "C3MFYU_SeQfx"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "batch_size = 8\n",
    "total = int(len(train_ds)/batch_size )\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "class_rep = defaultdict(int)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-6)\n",
    "train_dataloader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size = batch_size, shuffle = True)\n",
    "pbar = tqdm(enumerate(train_dataloader), total = total, leave = True, position = 0)\n",
    "acc_sum = 0\n",
    "\n",
    "for i, (x,y) in pbar:\n",
    "    if i >= total:\n",
    "        break\n",
    "    model.eval()\n",
    "    x_test, y_test = next(iter(test_dataloader))\n",
    "    test_acc = ((model(x_test.to(device)).argmax(dim =  -1) == y_test.to(device)).sum()).item()/batch_size\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    y = y.to(device)\n",
    "    yhat = model(x.to(device))\n",
    "    loss = criterion(yhat, y)\n",
    "    acc_sum += (yhat.argmax(dim =  -1) == y).sum()\n",
    "    class_rep[y[0].item()] += 1 # Incomplete\n",
    "    topk = np.where(np.argsort(yhat[0].cpu().detach().numpy()) == y[0].detach().cpu().numpy())[0][0] # TODO: Incomplete\n",
    "    \n",
    "    pbar.set_postfix_str(\"Loss: {:.2f} Test Acc: {:.2f} Acc: {:.2f} Top: {} Class: {}\".format(loss.item(),\n",
    "                                                                                               test_acc, \n",
    "                                                                             acc_sum.item()/float(batch_size * (i+1)), \n",
    "                                                                             topk, class_rep[y[0].item()]))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "collapsed": false,
    "id": "ul5_1EQo4ynl",
    "outputId": "6d8957e3-3276-4c16-9cd4-e8bc8a7c58f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1561/1562 [02:02<00:00, 12.76it/s, Loss: 3.81 Acc: 0.07 Top: 93 Class: 12]"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "batch_size = 32\n",
    "total = int(len(train_ds)/batch_size )\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "class_rep = defaultdict(int)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vision_model.parameters(), lr = 1e-2)\n",
    "train_dataloader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "pbar = tqdm(enumerate(train_dataloader), total = total, leave = True, position = 0)\n",
    "acc_sum = 0\n",
    "\n",
    "for i, (x,y) in pbar:\n",
    "    if i >= total:\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    y = y.to(device)\n",
    "    # x = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #         std=[0.229, 0.224, 0.225],)(x)\n",
    "    yhat = vision_model(x.to(device))\n",
    "    loss = criterion(yhat, y)\n",
    "    acc_sum += (yhat.argmax(dim =  -1) == y).sum()\n",
    "    class_rep[y[0].item()] += 1 # Incomplete\n",
    "    topk = np.where(np.argsort(yhat[0].cpu().detach().numpy()) == y[0].detach().cpu().numpy())[0][0] # TODO: Incomplete\n",
    "    pbar.set_postfix_str(\"Loss: {:.2f} Acc: {:.2f} Top: {} Class: {}\".format(loss.item(), \n",
    "                                                                            acc_sum.item()/float(batch_size * (i+1)), \n",
    "                                                                            topk, class_rep[y[0].item()]))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "-iOzYWVLeQf1"
   },
   "outputs": [],
   "source": [
    "y  = []\n",
    "z = []\n",
    "for x, _  in tqdm(train_ds):\n",
    "    z.append(x)\n",
    "    y.append((model(x.to(device).unsqueeze(0))).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "D-3XJcm1eQf5"
   },
   "outputs": [],
   "source": [
    "y = torch.stack(y)\n",
    "z = torch.stack(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "collapsed": false,
    "id": "-FqeYGjteQf9",
    "outputId": "825df2ce-a86d-427c-cbf8-afe92780eddb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1882, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.std()/y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "nQ5EySnveQgC"
   },
   "outputs": [],
   "source": [
    "i = np.random.choice(len(train_ds))\n",
    "print(F.softmax(model(train_ds[i][0].unsqueeze(0).to(device))))\n",
    "print(model(train_ds[i][0].unsqueeze(0).to(device)).argmax())\n",
    "print(train_ds[i][1])\n",
    "t = transforms.Compose([\n",
    "#     transforms.Grayscale(),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    ])(train_ds[i][0])\n",
    "print(F.softmax(model(t.unsqueeze(0).to(device))))\n",
    "print(model(t.unsqueeze(0).to(device)).argmax())\n",
    "\n",
    "print(F.softmax(model(((train_ds[i+1][0])).unsqueeze(0).to(device))))\n",
    "print(F.softmax(model(((train_ds[i+1][0])).unsqueeze(0).to(device))).argmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ncJnwGykeQgH"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict,strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zPjRCbmAeQgL"
   },
   "outputs": [],
   "source": [
    "model.toggleIntermediate()\n",
    "i = np.random.choice(len(train_ds))\n",
    "a1 = (F.softmax(model(train_ds[i][0].unsqueeze(0).to(device))))\n",
    "\n",
    "t = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(3),\n",
    "#     transforms.ColorJitter(10,10, 10),\n",
    "#     transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    ])(train_ds[i][0])\n",
    "a2 = (F.softmax(model(t.unsqueeze(0).to(device))))\n",
    "\n",
    "a3 = (F.softmax(model(((train_ds[i+1][0])).to(device).unsqueeze(0))))\n",
    "\n",
    "\n",
    "print(torch.norm(a1-a2, p = 2))\n",
    "print(torch.norm(a1-a3, p = 2))\n",
    "print(torch.norm(a2-a3, p = 2))\n",
    "model.toggleIntermediate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wks3jN_niRZ7"
   },
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert-vision.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bert-vision.py\n",
    "paperspace = True\n",
    "guy_folder = \"/content/\"\n",
    "if paperspace: \n",
    "    guy_folder = \"/notebooks/\"\n",
    "\n",
    "cache_dir = guy_folder+\"/cache/transformer_cache\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageNet, ImageFolder, CIFAR10, CIFAR100\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet101, resnet50\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "import wandb\n",
    "\n",
    "\n",
    "class DummyLayer(nn.Module):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__()\n",
    "    pass\n",
    "  def forward(self, x, *args, **kwargs):\n",
    "    return x\n",
    "\n",
    "class PlainBERT(nn.Module):\n",
    "    def __init__(self, n_tokens, min_layer = None):\n",
    "        super().__init__()\n",
    "        self.nLayers = 6\n",
    "        self.nHeads = 12\n",
    "        self.seqLen = 512\n",
    "\n",
    "\n",
    "        bert = AutoModel.from_pretrained('distilbert-base-uncased', cache_dir = cache_dir)\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.Tensor(bert.embeddings.position_embeddings(torch.arange(self.seqLen)).detach().numpy()))\n",
    "        if min_layer is None:\n",
    "          self.bert = bert.transformer\n",
    "        else:\n",
    "          raise NotImplementedError\n",
    "          bert_ = bert.transformer\n",
    "          for n, m in bert_.layer.named_children():\n",
    "            if int(n) < min_layer:\n",
    "              setattr(bert_.layer, n, DummyLayer())\n",
    "        \n",
    "          self.bert = bert_\n",
    "\n",
    "        self.bert.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bert.forward(x + self.position_embeddings, attn_mask = torch.ones(x.size(0), 512).to(x.device),\n",
    "                                head_mask = torch.ones(self.nLayers, x.size(0), \n",
    "                                                       self.nHeads, self.seqLen, self.seqLen).to(x.device))\n",
    "\n",
    "class BertVision(nn.Module):\n",
    "    def __init__(self,  n_classes, img_dim):\n",
    "        super().__init__()\n",
    "        self.with_classifier = True\n",
    "        self.n_tokens = np.prod(img_dim)\n",
    "        self.top = nn.Sequential(\n",
    "                                 nn.Conv2d(3, 32, 3, padding = 1 ),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(32, 100, 3, padding = 1),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(100, 200, 3, padding = 1),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(200, 768, 3, stride = (1, 2), padding = 1),\n",
    "                                 nn.LeakyReLU(0.2)\n",
    "                                )\n",
    "        \n",
    "        self.top.apply(self._init_top)\n",
    "        self.bert = PlainBERT(n_tokens = self.n_tokens)\n",
    "        self.fc = nn.Linear(768 * self.n_tokens//2, n_classes)\n",
    "        self.layer_norm = nn.LayerNorm((512,))\n",
    "\n",
    "    def toggleIntermediate(self):\n",
    "        self.with_classifier = not self.with_classifier\n",
    "    \n",
    "    \n",
    "    def _init_top(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        x = self.top(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.bert(x)\n",
    "        x = torch.stack(x).squeeze(0)\n",
    "        x = x.transpose(1,2).contiguous()\n",
    "#         x = self.layer_norm(x)\n",
    "#         x = torch.mean(x, dim = (-2,))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.with_classifier:\n",
    "            x = self.fc(x)\n",
    "        return x.squeeze(1)\n",
    "    \n",
    "    \n",
    "    \n",
    "device = 'cuda'\n",
    "train_ds = CIFAR100(\"{}/data/cifar100\".format(guy_folder), download = True, transform=transforms.ToTensor())\n",
    "test_ds = CIFAR100(\"{}/data/cifar100\".format(guy_folder), download = True, transform=transforms.ToTensor(), train = False)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "lr = {'bert-vision': [1e-6, 5e-6, 1e-5, 5e-5],\n",
    "      'resnet': [3e-4, 1e-3, 3e-3, 1e-2]\n",
    "      }\n",
    "\n",
    "optimizerDict = {'adam': torch.optim.Adam,\n",
    "                 'adamw': AdamW,\n",
    "                 'sgd': torch.optim.SGD, # No momentum\n",
    "                 }\n",
    "\n",
    "def makeModel(modelName):\n",
    "  if modelName == 'resnet':\n",
    "    model_resnet = resnet50(pretrained = True)\n",
    "    model_resnet.fc = nn.Linear(model_resnet.fc.in_features, 100)\n",
    "    model_resnet.to(device)\n",
    "    model = model_resnet\n",
    "  elif modelName == 'bert-vision':\n",
    "    model = BertVision(len(train_ds.classes), (32,32)).to(device)\n",
    "  else:\n",
    "    model = Sequential()\n",
    "  return model\n",
    "\n",
    "def train(config):\n",
    "  \n",
    "  optimizerAlg = optimizerDict[config.optimizer]\n",
    "  if config.model == 'bert-vision' and config.optimizer == 'adam':\n",
    "    optimizerAlg = optimizerDict['adamw']\n",
    "  modelName = config.model\n",
    "  lr_idx = config.lr_idx\n",
    "  model = makeModel(modelName)\n",
    "\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optimizerAlg(model.parameters(), lr = lr[modelName][lr_idx])\n",
    "  train_dataloader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "  test_dataloader = DataLoader(test_ds, batch_size = batch_size, shuffle = True)\n",
    "  pbar = tqdm(train_dataloader, leave = True, position = 0)\n",
    "  acc_sum = 0\n",
    "\n",
    "  for i, (x,y) in enumerate(pbar):\n",
    "      model.train()    \n",
    "      optimizer.zero_grad()\n",
    "      y = y.to(device)\n",
    "      yhat = model(x.to(device))\n",
    "      loss = criterion(yhat, y)\n",
    "      acc_sum += (yhat.argmax(dim =  -1) == y).sum()    \n",
    "      wandb.log({'loss': loss.item(), \n",
    "                 'acc': acc_sum.item() / (batch_size * (i+1))})\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "wandb.init()\n",
    "config = wandb.config\n",
    "train(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "id": "YJA22phbj1S6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: utw1dcw2\n",
      "Sweep URL: https://wandb.ai/dar-tau/bert-vision/sweeps/utw1dcw2\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'method': 'grid',\n",
    "    'program': 'bert-vision.py',\n",
    "    'parameters': \n",
    "    {\n",
    "        'lr_idx': {'values': [0, 1, 2, 3]},\n",
    "        'optimizer':{'values': ['adam', 'sgd']},\n",
    "        'model': {'values': ['resnet', 'bert-vision']}\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(config, project = 'bert-vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
      "2020-09-27 17:20:59,742 - wandb.wandb_agent - INFO - Running runs: []\n",
      "2020-09-27 17:20:59,935 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-09-27 17:20:59,935 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tlr_idx: 0\n",
      "\tmodel: resnet\n",
      "\toptimizer: adam\n",
      "2020-09-27 17:20:59,941 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python bert-vision.py --lr_idx=0 --model=resnet --optimizer=adam\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdar-tau\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2020-09-27 17:21:04,950 - wandb.wandb_agent - INFO - Running runs: ['mlhc6188']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200927_172104-mlhc6188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msandy-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision/sweeps/utw1dcw2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision/runs/mlhc6188\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 56.9MB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"bert-vision.py\", line 165, in <module>\n",
      "    train(config)\n",
      "  File \"bert-vision.py\", line 140, in train\n",
      "    model = makeModel(modelName)\n",
      "  File \"bert-vision.py\", line 125, in makeModel\n",
      "    model_resnet.to(device)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 432, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 208, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 230, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 430, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1.  Press ctrl-c to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20200927_172104-mlhc6188/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20200927_172104-mlhc6188/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msandy-sweep-1\u001b[0m: \u001b[34mhttps://wandb.ai/dar-tau/bert-vision/runs/mlhc6188\u001b[0m\n",
      "2020-09-27 17:21:15,222 - wandb.wandb_agent - INFO - Cleaning up finished run: mlhc6188\n",
      "2020-09-27 17:21:15,417 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-09-27 17:21:15,417 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tlr_idx: 0\n",
      "\tmodel: resnet\n",
      "\toptimizer: sgd\n",
      "2020-09-27 17:21:15,420 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python bert-vision.py --lr_idx=0 --model=resnet --optimizer=sgd\n",
      "^C\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl-c pressed. Waiting for runs to end. Press ctrl-c again to terminate them.\n"
     ]
    }
   ],
   "source": [
    "assert(sweep_id.isalnum())\n",
    "!wandb agent {sweep_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "bert-vision.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "guy-python",
   "language": "python",
   "name": "guy-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
