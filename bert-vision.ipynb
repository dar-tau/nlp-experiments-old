{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "guy_folder = \"/vol/scratch/guy\"\n",
    "cache_dir = guy_folder+\"/cache/transformer_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.3.2-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.11.0-py3-none-any.whl (283 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.1.2-cp38-cp38-manylinux1_x86_64.whl (10.4 MB)\n",
      "Requirement already satisfied: tqdm in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (4.46.0)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.2.0-cp38-cp38-manylinux1_x86_64.whl (92 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-7.2.0-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting numpy>=1.15\n",
      "  Using cached numpy-1.19.2-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting scipy>=1.0\n",
      "  Using cached scipy-1.5.2-cp38-cp38-manylinux1_x86_64.whl (25.7 MB)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from tensorboard) (2.23.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.21.2-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[K     |################################| 93 kB 192 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Using cached protobuf-3.13.0-cp38-cp38-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from tensorboard) (46.4.0.post20200518)\n",
      "Requirement already satisfied: six>=1.10.0 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from tensorboard) (1.14.0)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from tensorboard) (0.34.2)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: kiwisolver, pillow, cycler, numpy, matplotlib, scipy, pytz, pandas, seaborn, markdown, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, tensorboard-plugin-wit, protobuf, werkzeug, grpcio, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, tensorboard\n",
      "Successfully installed absl-py-0.10.0 cachetools-4.1.1 cycler-0.10.0 google-auth-1.21.2 google-auth-oauthlib-0.4.1 grpcio-1.32.0 kiwisolver-1.2.0 markdown-3.2.2 matplotlib-3.3.2 numpy-1.19.2 oauthlib-3.1.0 pandas-1.1.2 pillow-7.2.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytz-2020.1 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.5.2 seaborn-0.11.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 werkzeug-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/specific/scratches/scratch/guy\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.4.0+cu100\n",
      "  Downloading https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp38-cp38-linux_x86_64.whl (723.9 MB)\n",
      "\u001b[K     |################################| 723.9 MB 75.6 MB/s eta 0:00:01   |                                | 14.7 MB 2.3 MB/s eta 0:05:15     |###                             | 74.8 MB 46.5 MB/s eta 0:00:14     |########                        | 186.3 MB 53.2 MB/s eta 0:00:11     |###########                     | 258.2 MB 54.8 MB/s eta 0:00:09     |################                | 362.2 MB 18.4 MB/s eta 0:00:20     |###################             | 443.7 MB 10.1 MB/s eta 0:00:28     |####################            | 466.5 MB 52.3 MB/s eta 0:00:05     |######################          | 506.4 MB 30.9 MB/s eta 0:00:08     |###########################     | 624.6 MB 20.7 MB/s eta 0:00:05     |############################    | 633.9 MB 20.7 MB/s eta 0:00:05     |############################    | 654.7 MB 20.7 MB/s eta 0:00:04     |#############################   | 677.9 MB 34.3 MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-1.4.0+cu100\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/specific/scratches/scratch/guy/cache\n",
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 42744, done.\u001b[K\n",
      "remote: Total 42744 (delta 0), reused 0 (delta 0), pack-reused 42744\u001b[K\n",
      "Receiving objects: 100% (42744/42744), 30.66 MiB | 7.02 MiB/s, done.\n",
      "Resolving deltas: 100% (29585/29585), done.\n",
      "Checking connectivity... done.\n",
      "Checking out files: 100% (1214/1214), done.\n",
      "Processing ./transformers\n",
      "Requirement already satisfied: numpy in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from transformers==3.1.0) (1.19.2)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Using cached tokenizers-0.8.1rc2-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "Requirement already satisfied: packaging in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from transformers==3.1.0) (20.4)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: requests in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from transformers==3.1.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from transformers==3.1.0) (4.46.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2020.7.14-cp38-cp38-manylinux2010_x86_64.whl (672 kB)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Using cached sentencepiece-0.1.91-cp38-cp38-manylinux1_x86_64.whl (1.1 MB)\n",
      "Processing /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677/sacremoses-0.0.43-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from packaging->transformers==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: six in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from packaging->transformers==3.1.0) (1.14.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests->transformers==3.1.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests->transformers==3.1.0) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests->transformers==3.1.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests->transformers==3.1.0) (3.0.4)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-3.1.0-py3-none-any.whl size=996649 sha256=f27a26f5ebb0f915eb43bd013c59ca1ec2f90103379b63db69be969040539df9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-aks3vg3o/wheels/62/72/05/d86ccfc8a8e2bef63ff35c03f9c6ad973706febb4488bdd2f2\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, filelock, regex, sentencepiece, click, joblib, sacremoses, transformers\n",
      "Successfully installed click-7.1.2 filelock-3.0.12 joblib-0.16.0 regex-2020.7.14 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nlp\n",
      "  Using cached nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (1.1.2)\n",
      "Collecting pyarrow>=0.16.0\n",
      "  Using cached pyarrow-1.0.1-cp38-cp38-manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-2.0.0-cp38-cp38-manylinux2010_x86_64.whl (243 kB)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (4.46.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (3.0.12)\n",
      "Processing /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/93/7f/7d/78ec535a4340ef2696aad8b17fe8bb063d56301bd62881b069/dill-0.3.2-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from nlp) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from pandas->nlp) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from pandas->nlp) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.14.0)\n",
      "Installing collected packages: pyarrow, xxhash, dill, nlp\n",
      "Successfully installed dill-0.3.2 nlp-0.4.0 pyarrow-1.0.1 xxhash-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/specific/scratches/scratch/guy\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib seaborn pandas tqdm tensorboard\n",
    "\n",
    "%cd {guy_folder}\n",
    "!mkdir cache\n",
    "!mkdir cache/transformer_cache\n",
    "%pip install --no-cache-dir torch==1.4.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "%cd {guy_folder}/cache/\n",
    "!git clone https://github.com/huggingface/transformers.git\n",
    "%pip install ./transformers\n",
    "%pip install -U nlp\n",
    "\n",
    "%cd {guy_folder}\n",
    "\n",
    "## Not working!!!\n",
    "!setenv TRANSFORMERS_CACHE /vol/scratch/guy/cache/transformer_cache\n",
    "!setenv CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.7.0-cp38-cp38-manylinux1_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |################################| 5.9 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./miniconda/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Collecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp38-cp38-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |################################| 748.8 MB 7.3 kB/s  eta 0:00:01    |#                               | 38.3 MB 14.8 MB/s eta 0:00:49     |#                               | 43.3 MB 14.8 MB/s eta 0:00:48     |##                              | 67.9 MB 49.0 MB/s eta 0:00:14     |###                             | 76.4 MB 49.0 MB/s eta 0:00:14     |###                             | 89.5 MB 49.0 MB/s eta 0:00:14     |#####                           | 121.8 MB 60.5 MB/s eta 0:00:11     |######                          | 162.5 MB 49.5 MB/s eta 0:00:12     |#######                         | 177.7 MB 49.5 MB/s eta 0:00:12     |#######                         | 180.0 MB 49.5 MB/s eta 0:00:12     |########                        | 201.4 MB 38.5 MB/s eta 0:00:15     |###########                     | 274.4 MB 43.4 MB/s eta 0:00:11     |###############                 | 371.3 MB 44.4 MB/s eta 0:00:09     |#################               | 416.6 MB 14.5 MB/s eta 0:00:23     |##################              | 423.1 MB 14.5 MB/s eta 0:00:23     |###################             | 444.6 MB 50.0 MB/s eta 0:00:07     |###################             | 451.1 MB 50.0 MB/s eta 0:00:06     |#####################           | 507.1 MB 20.7 MB/s eta 0:00:12     |#######################         | 558.6 MB 42.0 MB/s eta 0:00:05     |########################        | 571.5 MB 25.2 MB/s eta 0:00:08     |##########################      | 621.1 MB 14.1 MB/s eta 0:00:10     |###########################     | 646.9 MB 46.0 MB/s eta 0:00:03     |###########################     | 648.8 MB 46.0 MB/s eta 0:00:03     |###########################     | 651.1 MB 46.0 MB/s eta 0:00:03     |###########################     | 653.3 MB 46.0 MB/s eta 0:00:03     |##############################  | 709.0 MB 16.3 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in ./miniconda/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |################################| 829 kB 94.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=4c35164ca92239c8bf68f9bd819ca484ef91556cf691b40db981ac6ebe3e184b\n",
      "  Stored in directory: /specific/a/home/cc/students/cs/dar/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "Successfully built future\n",
      "Installing collected packages: future, torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0+cu100\n",
      "    Uninstalling torch-1.4.0+cu100:\n",
      "      Successfully uninstalled torch-1.4.0+cu100\n",
      "Successfully installed future-0.18.2 torch-1.6.0 torchvision-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Might ruin things.\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-20 15:58:22--  http://www.image-net.org/archive/stanford/fall11_whole.tar\n",
      "Resolving www.image-net.org (www.image-net.org)... 171.64.68.16\n",
      "Connecting to www.image-net.org (www.image-net.org)|171.64.68.16|:80... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2020-09-20 15:58:22 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "# After login in through firefox to ImageNet\n",
    "%cd {guy_folder}/data\n",
    "!wget http://www.image-net.org/archive/stanford/fall11_whole.tar\n",
    "# !tar -xvf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageNet, ImageFolder, CIFAR10\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PlainBERT(nn.Module):\n",
    "    def __init__(self, n_tokens):\n",
    "        super().__init__()\n",
    "        bert = AutoModel.from_pretrained('distilbert-base-uncased', cache_dir = cache_dir)\n",
    "        self.position_embeddings = torch.Tensor(bert.embeddings.position_embeddings(torch.arange(512)).detach().numpy())\n",
    "        self.bert = bert.transformer\n",
    "        self.bert.requires_grad_(False)\n",
    "        self.nLayers = 6\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bert.forward(x + self.position_embeddings, attn_mask = torch.ones(x.shape[0], 512),\n",
    "                                head_mask = torch.ones(self.nLayers, x.shape[0], 512))\n",
    "\n",
    "class BertVision(nn.Module):\n",
    "    def __init__(self,  n_classes, img_dim):\n",
    "        super().__init__()\n",
    "        self.with_classifier = True\n",
    "        self.n_tokens = np.prod(img_dim)\n",
    "        self.top = nn.Sequential(\n",
    "                                 nn.Conv2d(3, 32, 3, padding = 1 ),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(32, 100, 3, padding = 1),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(100, 200, 3, padding = 1),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(200, 768, 3, stride = (1, 2), padding = 1),\n",
    "                                 nn.LeakyReLU(0.2)\n",
    "                                )\n",
    "        \n",
    "        self.top.apply(self._init_top)\n",
    "        self.bert = PlainBERT(n_tokens = self.n_tokens)\n",
    "        self.fc = nn.Linear(768 * self.n_tokens//2, n_classes)\n",
    "        self.layer_norm = nn.LayerNorm((512,))\n",
    "\n",
    "    def toggleIntermediate(self):\n",
    "        self.with_classifier = !self.with_classifier\n",
    "    \n",
    "    \n",
    "    def _init_top(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        x = self.top(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.bert(x)\n",
    "        x = torch.stack(x).squeeze(1)\n",
    "        x = x.transpose(1,2).contiguous()\n",
    "#         x = self.layer_norm(x)\n",
    "#         x = torch.mean(x, dim = (-2,))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.with_classifier:\n",
    "            x = self.fc(x)\n",
    "        return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_folder = \"{}/data/imagenet12/train\".format(guy_folder)\n",
    "\n",
    "train_ds = ImageFolder(train_folder,\n",
    "            transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_ds = CIFAR10(\"{}/data/cifar10\".format(guy_folder), download = True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = BertVision(len(train_ds.classes), (32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/5000 [00:56<39:11:52, 28.23s/it, Loss: 2.85 Acc: 0.33 Top: 4 Class: 1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-722-bb10a37a063b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                                              \u001b[0macc_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                                              topk, class_rep[y]))\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "total = int(len(train_ds) * .1)\n",
    "\n",
    "class_rep = defaultdict(int)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-6)\n",
    "train_dataloader = DataLoader(train_ds, batch_size = None, shuffle = True)\n",
    "pbar = tqdm(enumerate(train_dataloader), total = total)\n",
    "acc_sum = 0\n",
    "\n",
    "for i, (x,y) in pbar:\n",
    "    if i >= total:\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    yhat = model(x.unsqueeze(0))\n",
    "    loss = criterion(yhat, torch.LongTensor([y]))\n",
    "    acc_sum += yhat.argmax().item() == y\n",
    "    class_rep[y] += 1\n",
    "    topk = np.where(np.argsort(yhat.detach().numpy()) == y)[1][0]\n",
    "    pbar.set_postfix_str(\"Loss: {:.2f} Acc: {:.2f} Top: {} Class: {}\".format(loss.item(), \n",
    "                                                                             acc_sum/(i+1), \n",
    "                                                                             topk, class_rep[y]))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 56/50000 [00:12<3:03:01,  4.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-617-7e1060ce254b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-571-abb860736236>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-571-abb860736236>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         return self.bert.forward(x, attn_mask = torch.ones(x.shape[0], 512),\n\u001b[0m\u001b[1;32m     10\u001b[0m                                 head_mask = torch.ones(self.nLayers, x.shape[0], 512))\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    327\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             )\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/specific/scratches/scratch/guy/miniconda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y  = []\n",
    "z = []\n",
    "for x, _  in tqdm(train_ds):\n",
    "    z.append(x)\n",
    "    y.append((model(x.unsqueeze(0))).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = torch.stack(y)\n",
    "z = torch.stack(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1974)"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.std()/y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-713-fe7475450bf5>:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(model(train_ds[i][0].unsqueeze(0))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0854, 0.0993, 0.1222, 0.1293, 0.1071, 0.0606, 0.1577, 0.1181, 0.0355,\n",
      "         0.0847]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(6)\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-713-fe7475450bf5>:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(model(t.unsqueeze(0))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1851, 0.1524, 0.1046, 0.0928, 0.0759, 0.0412, 0.0796, 0.0728, 0.0872,\n",
      "         0.1084]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0)\n",
      "tensor([[0.1372, 0.2561, 0.0673, 0.0738, 0.0361, 0.0418, 0.0466, 0.0461, 0.1279,\n",
      "         0.1671]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-713-fe7475450bf5>:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(model(((train_ds[i+1][0])).unsqueeze(0))))\n",
      "<ipython-input-713-fe7475450bf5>:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(model(((train_ds[i+1][0])).unsqueeze(0))).argmax())\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(train_ds))\n",
    "print(F.softmax(model(train_ds[i][0].unsqueeze(0))))\n",
    "print(model(train_ds[i][0].unsqueeze(0)).argmax())\n",
    "print(train_ds[i][1])\n",
    "t = transforms.Compose([\n",
    "#     transforms.Grayscale(),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    ])(train_ds[i][0])\n",
    "print(F.softmax(model(t.unsqueeze(0))))\n",
    "print(model(t.unsqueeze(0)).argmax())\n",
    "\n",
    "print(F.softmax(model(((train_ds[i+1][0])).unsqueeze(0))))\n",
    "print(F.softmax(model(((train_ds[i+1][0])).unsqueeze(0))).argmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict,strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-721-f53303e89c02>:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a1 = (F.softmax(model(train_ds[i][0].unsqueeze(0))))\n",
      "<ipython-input-721-f53303e89c02>:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a2 = (F.softmax(model(t.unsqueeze(0))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4107, grad_fn=<NormBackward0>)\n",
      "tensor(0.6480, grad_fn=<NormBackward0>)\n",
      "tensor(0.2511, grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-721-f53303e89c02>:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a3 = (F.softmax(model(((train_ds[i+1][0])).unsqueeze(0))))\n"
     ]
    }
   ],
   "source": [
    "model.toggleIntermediate()\n",
    "i = np.random.choice(len(train_ds))\n",
    "a1 = (F.softmax(model(train_ds[i][0].unsqueeze(0))))\n",
    "\n",
    "t = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(3),\n",
    "#     transforms.ColorJitter(10,10, 10),\n",
    "#     transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    ])(train_ds[i][0])\n",
    "a2 = (F.softmax(model(t.unsqueeze(0))))\n",
    "\n",
    "a3 = (F.softmax(model(((train_ds[i+1][0])).unsqueeze(0))))\n",
    "\n",
    "\n",
    "print(torch.norm(a1-a2, p = 2))\n",
    "print(torch.norm(a1-a3, p = 2))\n",
    "print(torch.norm(a2-a3, p = 2))\n",
    "model.toggleIntermediate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guy-python",
   "language": "python",
   "name": "guy-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
