{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfj6qUCzeQfA"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "TMUnskJ9eQfB"
   },
   "outputs": [],
   "source": [
    "paperspace = True\n",
    "guy_folder = \"/content/\"\n",
    "if paperspace: \n",
    "    guy_folder = \"/notebooks/\"\n",
    "\n",
    "cache_dir = guy_folder+\"/cache/transformer_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling enum34-1.1.6:\n",
      "  Successfully uninstalled enum34-1.1.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y enum34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "NqAi8nT2eQfG",
    "outputId": "20ff04dc-620b-40f5-9427-af3192a9a444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "# %pip install matplotlib seaborn pandas tqdm tensorboard\n",
    "\n",
    "%cd {guy_folder}\n",
    "!mkdir cache\n",
    "!mkdir cache/transformer_cache\n",
    "# %pip install --no-cache-dir --upgrade torch torchvision\n",
    "#==1.4.0+cu100 torchvision== -f https://download.pytorch.org/whl/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "htC1bYBQfkz6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (0.10.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.24.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.0)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.0.0)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (5.1.2)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.17.8)\n",
      "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.3)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.8)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.7.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2020.6.20)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (41.0.1)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/notebooks/cache\n",
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
      "Processing ./transformers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (2.24.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (3.0.12)\n",
      "Collecting sacremoses (from transformers==3.2.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 13.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (0.7)\n",
      "Collecting tokenizers==0.8.1.rc2 (from transformers==3.2.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.0MB 27.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (1.17.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (4.49.0)\n",
      "Collecting sentencepiece!=0.1.92 (from transformers==3.2.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 35.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers==3.2.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/90/0dae9bdebd8f8f8a39f1b80fdef240bec36ff64359f5fc584034ce4633cf/regex-2020.9.27-cp36-cp36m-manylinux2010_x86_64.whl (662kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665kB 37.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging (from transformers==3.2.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.2.0) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.2.0) (0.13.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.2.0) (2.4.2)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-3.2.0-cp36-none-any.whl size=1058987 sha256=302398c62133a0a41f82452cc5e300157af6875a4486f077dd823f98fcc6e39c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vfkzwdgj/wheels/2a/5e/00/5e5e9c72c29da1e52bf9e486adc18e4e84470dcd7b154ebdaa\n",
      "Successfully built transformers\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=dc241bba608c03b99ceab3f5b6a32009b8b25bf5db5acbd885027e2b5428cafc\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, sacremoses, tokenizers, sentencepiece, packaging, transformers\n",
      "Successfully installed packaging-20.4 regex-2020.9.27 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.2.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already up-to-date: nlp in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.49.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: xxhash in /usr/local/lib/python3.6/dist-packages (from nlp) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n",
      "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from nlp) (0.25.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2019.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->nlp) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/notebooks\n",
      "/bin/sh: 1: setenv: not found\n",
      "/bin/sh: 1: setenv: not found\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb\n",
    "%cd {guy_folder}/cache/\n",
    "!git clone https://github.com/huggingface/transformers.git\n",
    "%pip install ./transformers\n",
    "%pip install -U nlp\n",
    "\n",
    "%cd {guy_folder}\n",
    "\n",
    "## Not working!!!\n",
    "!setenv TRANSFORMERS_CACHE /vol/scratch/guy/cache/transformer_cache\n",
    "!setenv CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KPmqdmNj6dN"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "XhRzzS1eeQfZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageNet, ImageFolder, CIFAR10, CIFAR100\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet101\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "sS-kATGqeQfc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DummyLayer(nn.Module):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__()\n",
    "    pass\n",
    "  def forward(self, x, *args, **kwargs):\n",
    "    return x\n",
    "\n",
    "class PlainBERT(nn.Module):\n",
    "    def __init__(self, n_tokens, min_layer = None):\n",
    "        super().__init__()\n",
    "        self.nLayers = 6\n",
    "        self.nHeads = 12\n",
    "        self.seqLen = 512\n",
    "\n",
    "\n",
    "        bert = AutoModel.from_pretrained('distilbert-base-uncased', cache_dir = cache_dir)\n",
    "        self.position_embeddings = nn.Parameter(torch.Tensor(bert.embeddings.position_embeddings(torch.arange(self.seqLen)).detach().numpy()))\n",
    "        if min_layer is None:\n",
    "          self.bert = bert.transformer\n",
    "        else:\n",
    "          raise NotImplementedError\n",
    "          bert_ = bert.transformer\n",
    "          for n, m in bert_.layer.named_children():\n",
    "            if int(n) < min_layer:\n",
    "              setattr(bert_.layer, n, DummyLayer())\n",
    "        \n",
    "          self.bert = bert_\n",
    "\n",
    "        self.bert.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bert.forward(x + self.position_embeddings, attn_mask = torch.ones(x.size(0), 512).to(x.device),\n",
    "                                head_mask = torch.ones(self.nLayers, x.size(0), self.nHeads, self.seqLen, self.seqLen).to(x.device))\n",
    "\n",
    "class BertVision(nn.Module):\n",
    "    def __init__(self,  n_classes, img_dim):\n",
    "        super().__init__()\n",
    "        self.with_classifier = True\n",
    "        self.n_tokens = np.prod(img_dim)\n",
    "        self.top = nn.Sequential(\n",
    "                                 nn.Conv2d(3, 32, 3, padding = 1 ),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(32, 100, 3, padding = 1),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(100, 200, 3, padding = 1),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(200, 768, 3, stride = (1, 2), padding = 1),\n",
    "                                 nn.LeakyReLU(0.2)\n",
    "                                )\n",
    "        \n",
    "        self.top.apply(self._init_top)\n",
    "        self.bert = PlainBERT(n_tokens = self.n_tokens)\n",
    "        self.fc = nn.Linear(768 * self.n_tokens//2, n_classes)\n",
    "        self.layer_norm = nn.LayerNorm((512,))\n",
    "\n",
    "    def toggleIntermediate(self):\n",
    "        self.with_classifier = !self.with_classifier\n",
    "    \n",
    "    \n",
    "    def _init_top(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        x = self.top(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.bert(x)\n",
    "        x = torch.stack(x).squeeze(0)\n",
    "        x = x.transpose(1,2).contiguous()\n",
    "#         x = self.layer_norm(x)\n",
    "#         x = torch.mean(x, dim = (-2,))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.with_classifier:\n",
    "            x = self.fc(x)\n",
    "        return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-Udifw4deQfn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /notebooks//data/cifar100/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169009152it [00:09, 18300545.88it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /notebooks//data/cifar100/cifar-100-python.tar.gz to /notebooks//data/cifar100\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "train_ds = CIFAR100(\"{}/data/cifar100\".format(guy_folder), download = True, transform=transforms.ToTensor())\n",
    "test_ds = CIFAR100(\"{}/data/cifar100\".format(guy_folder), download = True, transform=transforms.ToTensor(), train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QajOFf1ic7-"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ixPIuePqq86"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "# After login in through firefox to ImageNet\n",
    "%cd {guy_folder}/data\n",
    "!wget http://www.image-net.org/archive/stanford/fall11_whole.tar\n",
    "# !tar -xvf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnJGD7m7qfSw"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "train_folder = \"{}/data/imagenet12/train\".format(guy_folder)\n",
    "\n",
    "train_ds = ImageFolder(train_folder,\n",
    "            transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3MFYU_SeQfx"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "batch_size = 8\n",
    "total = int(len(train_ds)/batch_size )\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "class_rep = defaultdict(int)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-6)\n",
    "train_dataloader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size = batch_size, shuffle = True)\n",
    "pbar = tqdm(enumerate(train_dataloader), total = total, leave = True, position = 0)\n",
    "acc_sum = 0\n",
    "\n",
    "for i, (x,y) in pbar:\n",
    "    if i >= total:\n",
    "        break\n",
    "    model.eval()\n",
    "    x_test, y_test = next(iter(test_dataloader))\n",
    "    test_acc = ((model(x_test.to(device)).argmax(dim =  -1) == y_test.to(device)).sum()).item()/batch_size\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    y = y.to(device)\n",
    "    yhat = model(x.to(device))\n",
    "    loss = criterion(yhat, y)\n",
    "    acc_sum += (yhat.argmax(dim =  -1) == y).sum()\n",
    "    class_rep[y[0].item()] += 1 # Incomplete\n",
    "    topk = np.where(np.argsort(yhat[0].cpu().detach().numpy()) == y[0].detach().cpu().numpy())[0][0] # TODO: Incomplete\n",
    "    \n",
    "    pbar.set_postfix_str(\"Loss: {:.2f} Test Acc: {:.2f} Acc: {:.2f} Top: {} Class: {}\".format(loss.item(),\n",
    "                                                                                               test_acc, \n",
    "                                                                             acc_sum.item()/float(batch_size * (i+1)), \n",
    "                                                                             topk, class_rep[y[0].item()]))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "ul5_1EQo4ynl",
    "outputId": "6d8957e3-3276-4c16-9cd4-e8bc8a7c58f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1561/1562 [02:02<00:00, 12.76it/s, Loss: 3.81 Acc: 0.07 Top: 93 Class: 12]"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "batch_size = 32\n",
    "total = int(len(train_ds)/batch_size )\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "class_rep = defaultdict(int)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vision_model.parameters(), lr = 1e-2)\n",
    "train_dataloader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "pbar = tqdm(enumerate(train_dataloader), total = total, leave = True, position = 0)\n",
    "acc_sum = 0\n",
    "\n",
    "for i, (x,y) in pbar:\n",
    "    if i >= total:\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    y = y.to(device)\n",
    "    # x = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #         std=[0.229, 0.224, 0.225],)(x)\n",
    "    yhat = vision_model(x.to(device))\n",
    "    loss = criterion(yhat, y)\n",
    "    acc_sum += (yhat.argmax(dim =  -1) == y).sum()\n",
    "    class_rep[y[0].item()] += 1 # Incomplete\n",
    "    topk = np.where(np.argsort(yhat[0].cpu().detach().numpy()) == y[0].detach().cpu().numpy())[0][0] # TODO: Incomplete\n",
    "    pbar.set_postfix_str(\"Loss: {:.2f} Acc: {:.2f} Top: {} Class: {}\".format(loss.item(), \n",
    "                                                                            acc_sum.item()/float(batch_size * (i+1)), \n",
    "                                                                            topk, class_rep[y[0].item()]))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iOzYWVLeQf1"
   },
   "outputs": [],
   "source": [
    "y  = []\n",
    "z = []\n",
    "for x, _  in tqdm(train_ds):\n",
    "    z.append(x)\n",
    "    y.append((model(x.to(device).unsqueeze(0))).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-3XJcm1eQf5"
   },
   "outputs": [],
   "source": [
    "y = torch.stack(y)\n",
    "z = torch.stack(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "-FqeYGjteQf9",
    "outputId": "825df2ce-a86d-427c-cbf8-afe92780eddb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1882, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.std()/y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ5EySnveQgC"
   },
   "outputs": [],
   "source": [
    "i = np.random.choice(len(train_ds))\n",
    "print(F.softmax(model(train_ds[i][0].unsqueeze(0).to(device))))\n",
    "print(model(train_ds[i][0].unsqueeze(0).to(device)).argmax())\n",
    "print(train_ds[i][1])\n",
    "t = transforms.Compose([\n",
    "#     transforms.Grayscale(),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    ])(train_ds[i][0])\n",
    "print(F.softmax(model(t.unsqueeze(0).to(device))))\n",
    "print(model(t.unsqueeze(0).to(device)).argmax())\n",
    "\n",
    "print(F.softmax(model(((train_ds[i+1][0])).unsqueeze(0).to(device))))\n",
    "print(F.softmax(model(((train_ds[i+1][0])).unsqueeze(0).to(device))).argmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncJnwGykeQgH"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict,strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPjRCbmAeQgL"
   },
   "outputs": [],
   "source": [
    "model.toggleIntermediate()\n",
    "i = np.random.choice(len(train_ds))\n",
    "a1 = (F.softmax(model(train_ds[i][0].unsqueeze(0).to(device))))\n",
    "\n",
    "t = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(3),\n",
    "#     transforms.ColorJitter(10,10, 10),\n",
    "#     transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    ])(train_ds[i][0])\n",
    "a2 = (F.softmax(model(t.unsqueeze(0).to(device))))\n",
    "\n",
    "a3 = (F.softmax(model(((train_ds[i+1][0])).to(device).unsqueeze(0))))\n",
    "\n",
    "\n",
    "print(torch.norm(a1-a2, p = 2))\n",
    "print(torch.norm(a1-a3, p = 2))\n",
    "print(torch.norm(a2-a3, p = 2))\n",
    "model.toggleIntermediate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wks3jN_niRZ7"
   },
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "YJA22phbj1S6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: yixl1a2n\n",
      "Sweep URL: https://wandb.ai/dar-tau/bert-vision/sweeps/yixl1a2n\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'method': 'grid',\n",
    "    'program': 'bert-vision.py',\n",
    "    'parameters': \n",
    "    {\n",
    "        'lr_idx': {'values': [0, 1, 2, 3, 4]},\n",
    "        'optimizer':{'values': ['adam', 'adamw', 'sgd']},\n",
    "        'model': {'values': ['bert-vision', 'resnet101']}\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent üïµÔ∏è\n",
      "2020-09-27 16:11:19,399 - wandb.wandb_agent - INFO - Running runs: []\n",
      "2020-09-27 16:11:19,754 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-09-27 16:11:19,754 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tlr_idx: 0\n",
      "\tmodel: bert-vision\n",
      "\toptimizer: adam\n",
      "2020-09-27 16:11:19,764 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python bert-vision.py --lr_idx=0 --model=bert-vision --optimizer=adam\n",
      "Files already downloaded and verified\n",
      "2020-09-27 16:11:24,778 - wandb.wandb_agent - INFO - Running runs: ['pojpcjpm']\n",
      "Files already downloaded and verified\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdar-tau\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200927_161125-pojpcjpm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcerulean-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision/sweeps/yixl1a2n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision/runs/pojpcjpm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "{'lr_idx': 0, 'model': 'bert-vision', 'optimizer': 'adam'}\n",
      "2020-09-27 16:11:37,386 - wandb.wandb_agent - INFO - Cleaning up finished run: pojpcjpm\n",
      "2020-09-27 16:11:37,622 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-09-27 16:11:37,622 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tlr_idx: 0\n",
      "\tmodel: bert-vision\n",
      "\toptimizer: adamw\n",
      "2020-09-27 16:11:37,626 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python bert-vision.py --lr_idx=0 --model=bert-vision --optimizer=adamw\n",
      "Files already downloaded and verified\n",
      "2020-09-27 16:11:42,640 - wandb.wandb_agent - INFO - Running runs: ['bl0os9vc']\n",
      "Files already downloaded and verified\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdar-tau\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200927_161144-bl0os9vc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstoic-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision/sweeps/yixl1a2n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision/runs/bl0os9vc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "{'lr_idx': 0, 'model': 'bert-vision', 'optimizer': 'adamw'}\n",
      "2020-09-27 16:11:52,926 - wandb.wandb_agent - INFO - Cleaning up finished run: bl0os9vc\n",
      "2020-09-27 16:11:53,300 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-09-27 16:11:53,301 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tlr_idx: 0\n",
      "\tmodel: bert-vision\n",
      "\toptimizer: sgd\n",
      "2020-09-27 16:11:53,303 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python bert-vision.py --lr_idx=0 --model=bert-vision --optimizer=sgd\n",
      "Files already downloaded and verified\n",
      "2020-09-27 16:11:58,316 - wandb.wandb_agent - INFO - Running runs: ['56wm29am']\n",
      "Files already downloaded and verified\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdar-tau\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200927_161200-56wm29am\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwise-sweep-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision/sweeps/yixl1a2n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dar-tau/bert-vision/runs/56wm29am\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "{'lr_idx': 0, 'model': 'bert-vision', 'optimizer': 'sgd'}\n",
      "^C\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl-c pressed. Waiting for runs to end. Press ctrl-c again to terminate them.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n"
     ]
    }
   ],
   "source": [
    "assert(sweep_id.isalnum())\n",
    "!wandb agent {sweep_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweep train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "lr = {'bert-vision': [3e-7, 1e-6, 5e-6, 1e-5, 5e-5],\n",
    "      'resnet101': [1e-4, 3e-4, 1e-3, 3e-3, 1e-2]\n",
    "      }\n",
    "\n",
    "optimizerDict = {'adam': torch.optim.Adam,\n",
    "                 'adamw': AdamW,\n",
    "                 'sgd': torch.optim.SGD, # No momentum\n",
    "                 }\n",
    "\n",
    "def makeModel(modelName):\n",
    "  if modelName == 'resnet101':\n",
    "    model_resnet101 = resnet101(pretrained = True)\n",
    "    model_resnet101.fc = nn.Linear(vision_model.fc.in_features, 100)\n",
    "    model_resnet101.to(device)\n",
    "    model = model_resnet101\n",
    "  elif modelName == 'bert-vision':\n",
    "    model = BertVision(len(train_ds.classes), (32,32)).to(device)\n",
    "  else:\n",
    "    model = Sequential()\n",
    "  return model\n",
    "\n",
    "def train():\n",
    "  wandb_run = wandb.init()\n",
    "  config = wandb_run.config\n",
    "  return\n",
    "  optimizerAlg = optimizerDict[config.optimizer]\n",
    "  modelName = config.model\n",
    "  lr_idx = config.lr_idx\n",
    "  model = makeModel(modelName)\n",
    "\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optimizerAlg(model.parameters(), lr = lr[modelName][lr_idx])\n",
    "  train_dataloader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "  test_dataloader = DataLoader(test_ds, batch_size = batch_size, shuffle = True)\n",
    "  pbar = tqdm(train_dataloader, leave = True, position = 0)\n",
    "  acc_sum = 0\n",
    "\n",
    "  for i, (x,y) in enumerate(pbar):\n",
    "      model.train()    \n",
    "      optimizer.zero_grad()\n",
    "      y = y.to(device)\n",
    "      yhat = model(x.to(device))\n",
    "      loss = criterion(yhat, y)\n",
    "      acc_sum += (yhat.argmax(dim =  -1) == y).sum()    \n",
    "      wandb.log({'loss': loss.item(), \n",
    "                 'acc': acc_sum.item()/(i+1)})\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "bert-vision.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
